The original hyperlink graph dataset is formatted as a series of text files
stored in the same folder. On each line of each file is a pair of tab-separated
integers that represent the source node and destination node for a link in the
graph.

It is available with the following command:

wget -i http://webdatacommons.org/hyperlinkgraph/2014-04/data/arc.list.txt

That will download a collection of gzip'ed text files. You will need to iterate
over them and run gzip -d to extract them.

That extraction should give you about 1TB of data. To deal with more reasonable
dataset sizes for testing, just trim out some of the text files. For example, to
test with ~1 billion elements only requires ~10-20 text files usually.

Then, run the scripts in this
directory on those files to produce a working dataset.
